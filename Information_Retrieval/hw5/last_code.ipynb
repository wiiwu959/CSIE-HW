{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T13:28:54.641699Z",
     "iopub.status.busy": "2020-12-17T13:28:54.641183Z",
     "iopub.status.idle": "2020-12-17T13:28:55.211210Z",
     "shell.execute_reply": "2020-12-17T13:28:55.210315Z",
     "shell.execute_reply.started": "2020-12-17T13:28:54.641565Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T13:28:55.212509Z",
     "iopub.status.busy": "2020-12-17T13:28:55.212353Z",
     "iopub.status.idle": "2020-12-17T13:28:55.221765Z",
     "shell.execute_reply": "2020-12-17T13:28:55.221023Z",
     "shell.execute_reply.started": "2020-12-17T13:28:55.212489Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./ntust-ir-2020_hw5_new/query_list.txt', 'r') as fp:\n",
    "    query_list = [line.rstrip() for line in fp]\n",
    "\n",
    "with open('./ntust-ir-2020_hw5_new/doc_list.txt', 'r') as fp:\n",
    "    doc_list = [line.rstrip() for line in fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T13:28:55.482619Z",
     "iopub.status.busy": "2020-12-17T13:28:55.482147Z",
     "iopub.status.idle": "2020-12-17T13:28:55.924457Z",
     "shell.execute_reply": "2020-12-17T13:28:55.923568Z",
     "shell.execute_reply.started": "2020-12-17T13:28:55.482561Z"
    }
   },
   "outputs": [],
   "source": [
    "documents, queries = [], []\n",
    "\n",
    "for doc_name in doc_list:\n",
    "    with open('./ntust-ir-2020_hw5_new/docs/' + doc_name + '.txt', 'r') as fp:\n",
    "        doc_content = fp.read()\n",
    "        documents.append(doc_content)\n",
    "\n",
    "for query_name in query_list:\n",
    "    with open('./ntust-ir-2020_hw5_new/queries/' + query_name + '.txt', 'r') as fp:\n",
    "        query_content = fp.read()\n",
    "        queries.append(query_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T13:28:57.289034Z",
     "iopub.status.busy": "2020-12-17T13:28:57.288703Z",
     "iopub.status.idle": "2020-12-17T13:29:18.218213Z",
     "shell.execute_reply": "2020-12-17T13:29:18.217361Z",
     "shell.execute_reply.started": "2020-12-17T13:28:57.288994Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(smooth_idf=True, sublinear_tf=True)\n",
    "vectorizer.fit(queries + documents)\n",
    "doc_tfidf = vectorizer.fit_transform(documents).toarray()\n",
    "query_tfidf = vectorizer.transform(queries).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T13:29:18.219617Z",
     "iopub.status.busy": "2020-12-17T13:29:18.219447Z",
     "iopub.status.idle": "2020-12-17T13:29:51.170868Z",
     "shell.execute_reply": "2020-12-17T13:29:51.169950Z",
     "shell.execute_reply.started": "2020-12-17T13:29:18.219598Z"
    }
   },
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(query_tfidf, doc_tfidf)\n",
    "result = np.flip(cos_sim.argsort(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origin VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:37:43.514930Z",
     "iopub.status.busy": "2020-12-17T14:37:43.514442Z",
     "iopub.status.idle": "2020-12-17T14:38:08.648338Z",
     "shell.execute_reply": "2020-12-17T14:38:08.647661Z",
     "shell.execute_reply.started": "2020-12-17T14:37:43.514873Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, math\n",
    "from math import log, pow, sqrt\n",
    "from numba import jit\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('./ntust-ir-2020_hw5_new/query_list.txt', 'r') as fp:\n",
    "    query_list = fp.read()\n",
    "query_list = query_list.splitlines()\n",
    "\n",
    "with open('./ntust-ir-2020_hw5_new/doc_list.txt', 'r') as fp:\n",
    "    doc_list = fp.read()\n",
    "doc_list = doc_list.splitlines()\n",
    "\n",
    "\n",
    "total_word_count = 0\n",
    "\n",
    "word_doc_num = {}\n",
    "doc_word = {}\n",
    "for doc in doc_list:\n",
    "    with open('./ntust-ir-2020_hw5_new/docs/' + doc + '.txt', 'r') as fp:\n",
    "        doc_content = fp.read()\n",
    "    doc_content = doc_content.split(' ')\n",
    "    doc_word[doc] = doc_content\n",
    "    for word in doc_content:\n",
    "        if word not in word_doc_num:\n",
    "            word_doc_num[word] = {}\n",
    "        if doc not in  word_doc_num[word]:\n",
    "            word_doc_num[word][doc] = 0\n",
    "        word_doc_num[word][doc] += 1\n",
    "    total_word_count += len(doc_content)\n",
    "        \n",
    "word_query_num = {}\n",
    "query_word = {}\n",
    "for query in query_list:\n",
    "    with open('./ntust-ir-2020_hw5_new/queries/' + query + '.txt', 'r') as fp:\n",
    "        query_content = fp.read()\n",
    "    query_content = query_content.split(' ')\n",
    "    query_word[query] = query_content\n",
    "    for word in query_content:\n",
    "        if word not in word_query_num:\n",
    "            word_query_num[word] = {}\n",
    "        if query not in  word_query_num[word]:\n",
    "            word_query_num[word][query] = 0\n",
    "        word_query_num[word][query] += 1\n",
    "        \n",
    "        \n",
    "# make a list of all words\n",
    "total_word = []\n",
    "for word, num in word_query_num.items():\n",
    "    total_word.append(word)\n",
    "for word, num in word_doc_num.items():\n",
    "    total_word.append(word)\n",
    "total_word = list(dict.fromkeys(total_word))\n",
    "\n",
    "# count max df\n",
    "max_df = 0\n",
    "for a, b in word_doc_num.items():\n",
    "    if len(b) > max_df:\n",
    "        max_df = len(b)\n",
    "        \n",
    "        \n",
    "# count tfidf of each word in all of the doc\n",
    "doc_word_tfidf = {}\n",
    "doc_length = {}\n",
    "for doc, content in doc_word.items():\n",
    "    doc_word_tfidf[doc] = {}\n",
    "    doc_len_2 = 0\n",
    "    for word in content:\n",
    "        if word not in doc_word_tfidf[doc]:\n",
    "            # 0.70924\n",
    "            tf = 1 + log(word_doc_num[word][doc])\n",
    "            idf = log(1 + max_df / len(word_doc_num[word]))\n",
    "            \n",
    "            doc_word_tfidf[doc][word] = tf * idf\n",
    "            doc_len_2 += pow(tf * idf, 2)\n",
    "    doc_length[doc] = sqrt(doc_len_2)\n",
    "    \n",
    "# count tfidf of each word in all of the query\n",
    "query_word_tfidf = {}\n",
    "query_length = {}\n",
    "for query, content in query_word.items():\n",
    "    query_word_tfidf[query] = {}\n",
    "    query_len_2 = 0\n",
    "    for word in content:\n",
    "        if word not in query_word_tfidf[query]:\n",
    "            # 0.70924\n",
    "            tf = 1 + log(word_query_num[word][query])\n",
    "            idf = log(1 + max_df / len(word_doc_num[word])) if word in word_doc_num else 0\n",
    "            \n",
    "            query_word_tfidf[query][word] = tf * idf\n",
    "            query_len_2 += pow(tf * idf, 2)\n",
    "    query_length[query] = sqrt(query_len_2)\n",
    "    \n",
    "# count the cosine using data we pre-calculate before\n",
    "first_result = {}\n",
    "for query in query_list:\n",
    "    for doc in doc_list:\n",
    "        cossim = 0\n",
    "        for word, tfidf in query_word_tfidf[query].items():\n",
    "            cossim = cossim + (tfidf * doc_word_tfidf[doc][word]) if word in doc_word_tfidf[doc] else cossim\n",
    "        if query not in first_result:\n",
    "            first_result[query] = {}\n",
    "        first_result[query][doc] = cossim / (doc_length[doc] * query_length[query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T08:00:14.924667Z",
     "iopub.status.busy": "2020-12-17T08:00:14.924177Z",
     "iopub.status.idle": "2020-12-17T08:00:14.930932Z",
     "shell.execute_reply": "2020-12-17T08:00:14.929565Z",
     "shell.execute_reply.started": "2020-12-17T08:00:14.924612Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 0.9\n",
    "gamma = 0.3\n",
    "select_rel = 5\n",
    "select_nonrel = 3\n",
    "round_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T08:00:16.451101Z",
     "iopub.status.busy": "2020-12-17T08:00:16.450833Z",
     "iopub.status.idle": "2020-12-17T08:02:00.147258Z",
     "shell.execute_reply": "2020-12-17T08:02:00.146640Z",
     "shell.execute_reply.started": "2020-12-17T08:00:16.451073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:43<00:00, 34.56s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(round_num)):\n",
    "\n",
    "    rel_vecs = doc_tfidf[result[:, :select_rel]].mean(axis=1)\n",
    "    nrel_vecs = doc_tfidf[result[:, -select_nonrel:]].mean(axis=1)\n",
    "    query_tfidf = alpha * query_tfidf + beta * rel_vecs - gamma * nrel_vecs\n",
    "    \n",
    "    cos_sim = cosine_similarity(query_tfidf, doc_tfidf)\n",
    "    result = np.flip(cos_sim.argsort(axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T08:02:00.148384Z",
     "iopub.status.busy": "2020-12-17T08:02:00.148234Z",
     "iopub.status.idle": "2020-12-17T08:02:00.946265Z",
     "shell.execute_reply": "2020-12-17T08:02:00.945810Z",
     "shell.execute_reply.started": "2020-12-17T08:02:00.148365Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('SMM_Rocchio.txt', 'w+')\n",
    "f.write('Query,RetrievedDocuments\\n')\n",
    "for query_name, query_result in zip(query_list, result):\n",
    "    doc_result = ' '.join([doc_list[i] for i in query_result])\n",
    "    f.write('%s,%s\\n' % (query_name, doc_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:38:12.886279Z",
     "iopub.status.busy": "2020-12-17T14:38:12.885930Z",
     "iopub.status.idle": "2020-12-17T14:38:12.891539Z",
     "shell.execute_reply": "2020-12-17T14:38:12.890679Z",
     "shell.execute_reply.started": "2020-12-17T14:38:12.886238Z"
    }
   },
   "outputs": [],
   "source": [
    "smm_alpha = 0.9\n",
    "alpha = 0.3\n",
    "beta = 0.8\n",
    "gamma = 0.7\n",
    "select_rel = 5\n",
    "e_m_round = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:38:14.344024Z",
     "iopub.status.busy": "2020-12-17T14:38:14.343551Z",
     "iopub.status.idle": "2020-12-17T14:38:34.227214Z",
     "shell.execute_reply": "2020-12-17T14:38:34.225991Z",
     "shell.execute_reply.started": "2020-12-17T14:38:14.343968Z"
    }
   },
   "outputs": [],
   "source": [
    "new_queries = []\n",
    "\n",
    "for query_result in result:\n",
    "    new_queries.append(' '.join([documents[i] for i in query_result[:select_rel]]))\n",
    "\n",
    "# for query_name, query_result in first_result.items():\n",
    "#     temp = sorted(query_result.items(), key=lambda x:x[1],reverse=True)\n",
    "#     new_queries.append(' '.join([' '.join(doc_word[i[0]]) for i in temp[:select_rel]]))\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(queries + documents)\n",
    "query_tf = counter.transform(queries).toarray()\n",
    "new_query_tf = counter.transform(new_queries).toarray()\n",
    "doc_tf = counter.transform(documents).toarray()\n",
    "\n",
    "word_list = counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:38:34.228615Z",
     "iopub.status.busy": "2020-12-17T14:38:34.228458Z",
     "iopub.status.idle": "2020-12-17T14:39:14.466214Z",
     "shell.execute_reply": "2020-12-17T14:39:14.465620Z",
     "shell.execute_reply.started": "2020-12-17T14:38:34.228596Z"
    }
   },
   "outputs": [],
   "source": [
    "# init possibility\n",
    "def get_random_p(row, col):\n",
    "    np.random.seed(314)\n",
    "    temp = np.abs(np.random.randn(row, col))\n",
    "    temp /= temp.sum(axis=1, keepdims=True)\n",
    "    return temp\n",
    "\n",
    "P_SMM_w = get_random_p(len(query_list),len(word_list)) # 150 * 154209\n",
    "P_w_BG = doc_tf.sum(axis = 0) / doc_tf.sum() # 154209 one-vector\n",
    "P_ULM_w_q = query_tf / query_tf.sum(axis=1).reshape(-1,1) # 150 * 154209\n",
    "P_ULM_w_dj = doc_tf / doc_tf.sum(axis=1).reshape(-1,1) # 150 * 154209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:39:14.467364Z",
     "iopub.status.busy": "2020-12-17T14:39:14.467216Z",
     "iopub.status.idle": "2020-12-17T14:39:14.477829Z",
     "shell.execute_reply": "2020-12-17T14:39:14.477266Z",
     "shell.execute_reply.started": "2020-12-17T14:39:14.467346Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def e_m_step(P_SMM_w, P_w_BG):\n",
    "    P_T_SMM_w = ((1 - smm_alpha) * P_SMM_w) / ((1 - smm_alpha) * P_SMM_w + smm_alpha * P_w_BG)  # 150 * 154209\n",
    "#     P_SMM_w = (new_query_tf * P_T_SMM_w) / ((new_query_tf * P_T_SMM_w).sum(axis = 1).reshape(-1,1))  \n",
    "    P_SMM_w = (new_query_tf * P_T_SMM_w) / P_T_SMM_w.sum(axis=1).reshape(-1,1)\n",
    "    return P_SMM_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:39:14.478764Z",
     "iopub.status.busy": "2020-12-17T14:39:14.478631Z",
     "iopub.status.idle": "2020-12-17T14:39:14.482400Z",
     "shell.execute_reply": "2020-12-17T14:39:14.481829Z",
     "shell.execute_reply.started": "2020-12-17T14:39:14.478748Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def kl_divergent(P_ULM_w_q, P_ULM_w_dj, P_SMM_w, P_w_BG):\n",
    "    P_w_q = alpha * P_ULM_w_q + beta * P_SMM_w + (1 - alpha - beta) * P_w_BG # 150 * 154209\n",
    "    P_w_dj = gamma * P_ULM_w_dj + (1 - gamma) * P_w_BG # 30000 * 154209\n",
    "    return (P_w_q.dot(np.log(P_w_dj).T)) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:39:14.483137Z",
     "iopub.status.busy": "2020-12-17T14:39:14.483014Z",
     "iopub.status.idle": "2020-12-17T14:39:18.841256Z",
     "shell.execute_reply": "2020-12-17T14:39:18.840417Z",
     "shell.execute_reply.started": "2020-12-17T14:39:14.483122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(e_m_round)):\n",
    "    P_SMM_w = e_m_step(P_SMM_w, P_w_BG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:39:18.842572Z",
     "iopub.status.busy": "2020-12-17T14:39:18.842387Z",
     "iopub.status.idle": "2020-12-17T14:41:26.891336Z",
     "shell.execute_reply": "2020-12-17T14:41:26.890203Z",
     "shell.execute_reply.started": "2020-12-17T14:39:18.842549Z"
    }
   },
   "outputs": [],
   "source": [
    "last_result = kl_divergent(P_ULM_w_q, P_ULM_w_dj, P_SMM_w, P_w_BG)\n",
    "last_result = np.argsort(last_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-17T14:41:26.892648Z",
     "iopub.status.busy": "2020-12-17T14:41:26.892478Z",
     "iopub.status.idle": "2020-12-17T14:41:27.761252Z",
     "shell.execute_reply": "2020-12-17T14:41:27.760714Z",
     "shell.execute_reply.started": "2020-12-17T14:41:26.892628Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('test.txt', 'w+')\n",
    "f.write('Query,RetrievedDocuments\\n')\n",
    "for query_name, query_result in zip(query_list, last_result):\n",
    "    doc_result = ' '.join([doc_list[i] for i in query_result])\n",
    "    f.write('%s,%s\\n' % (query_name, doc_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
